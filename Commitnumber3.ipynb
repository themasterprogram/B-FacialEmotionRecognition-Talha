{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themasterprogram/B-FacialEmotionRecognition-Talha/blob/main/Commitnumber3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Whk-EbHSMAYR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.regularizers import l1, l2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l1, l2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alR2RSWyQgR4",
        "outputId": "0b2e4a9c-47f3-482f-eeed-00d9be1ef1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pHQ_E4LRSe7",
        "outputId": "4d02b095-cef4-4d1e-fea1-73f0a92a60c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1MNW2riRc2v",
        "outputId": "347283b6-5801-47da-eeca-91ebfb708cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Drive/MyDrive/fer/fer2013/fer2013\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Drive/MyDrive/fer/fer2013/fer2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8285tkzQMOCJ"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "df = pd.read_csv(\"fer2013.csv\")\n",
        "for index, row in df.iterrows():\n",
        "    k = row['pixels'].split(\" \")\n",
        "    if row['Usage'] == 'Training':\n",
        "        X_train.append(np.array(k))\n",
        "        y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == 'PublicTest':\n",
        "        X_test.append(np.array(k))\n",
        "        y_test.append(row['emotion'])\n",
        "\n",
        "X_train = np.array(X_train, dtype = 'uint8')\n",
        "y_train = np.array(y_train, dtype = 'uint8')\n",
        "X_test = np.array(X_test, dtype = 'uint8')\n",
        "y_test = np.array(y_test, dtype = 'uint8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i05TWHIvSBjE"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "y_train= to_categorical(y_train, num_classes=7)\n",
        "y_test = to_categorical(y_test, num_classes=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F90pEIMJRqu4"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MXg1PhIQjkXv"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range = 10,\n",
        "    horizontal_flip = True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode = 'nearest')\n",
        "testgen = ImageDataGenerator(rescale=1./255)\n",
        "datagen.fit(X_train)\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA6KmQ9UlAgD"
      },
      "outputs": [],
      "source": [
        "train_flow = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yCdEfF0tlXvq"
      },
      "outputs": [],
      "source": [
        "# Generate the model\n",
        "model = Sequential()\n",
        "# Layer 1: Convolutional\n",
        "model.add(Conv2D(input_shape=(224, 224, 3), filters=64, kernel_size=(3, 3),\n",
        "                 padding='same', activation='relu'))\n",
        "# Layer 2: Convolutional\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 3: MaxPooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Layer 4: Convolutional\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 5: Convolutional\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 6: MaxPooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Layer 7: Convolutional\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 8: Convolutional\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 9: Convolutional\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 10: MaxPooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Layer 11: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 12: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 13: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 14: MaxPooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Layer 15: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 16: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 17: Convolutional\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# Layer 18: MaxPooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Layer 19: Flatten\n",
        "model.add(Flatten())\n",
        "# Layer 20: Fully Connected Layer\n",
        "model.add(Dense(units=4096, activation='relu'))\n",
        "# Layer 21: Fully Connected Layer\n",
        "model.add(Dense(units=4096, activation='relu'))\n",
        "# Layer 22: Softmax Layer\n",
        "model.add(Dense(units=2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIM9Nb0Zl2S8",
        "outputId": "53fec1ba-4c36-4aeb-e8be-a32c54d765a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 48, 48, 1)]       0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 46, 46, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 23, 23, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 21, 21, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 21, 21, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 20, 20, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 18, 18, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 18, 18, 256)       1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPooli  (None, 9, 9, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " drop1 (Dropout)             (None, 9, 9, 256)         0         \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (None, 9, 9, 512)         1180160   \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 9, 9, 512)         2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 8, 8, 512)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (None, 8, 8, 1024)        4719616   \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 8, 8, 1024)        4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 4, 4, 1024)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " drop2 (Dropout)             (None, 4, 4, 1024)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " Dense1 (Dense)              (None, 2048)              33556480  \n",
            "                                                                 \n",
            " Dense2 (Dense)              (None, 1024)              2098176   \n",
            "                                                                 \n",
            " Dense3 (Dense)              (None, 512)               524800    \n",
            "                                                                 \n",
            " Dense4 (Dense)              (None, 256)               131328    \n",
            "                                                                 \n",
            " output (Dense)              (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42589959 (162.47 MB)\n",
            "Trainable params: 42585991 (162.45 MB)\n",
            "Non-trainable params: 3968 (15.50 KB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#model = FER_Model()\n",
        "#opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ9QTm5dmwhZ",
        "outputId": "89a26465-6e57-4827-f923-9dc157311d76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-37e6bcd0ecfb>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_flow,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "448/448 [==============================] - 2891s 6s/step - loss: 1.6743 - accuracy: 0.3356 - val_loss: 2.3323 - val_accuracy: 0.1953\n",
            "Epoch 2/100\n",
            "448/448 [==============================] - 2915s 7s/step - loss: 1.4318 - accuracy: 0.4446 - val_loss: 1.4256 - val_accuracy: 0.4595\n",
            "Epoch 3/100\n",
            "448/448 [==============================] - 2830s 6s/step - loss: 1.3026 - accuracy: 0.5005 - val_loss: 1.2406 - val_accuracy: 0.5322\n",
            "Epoch 4/100\n",
            "351/448 [======================>.......] - ETA: 9:47 - loss: 1.2263 - accuracy: 0.5308"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "history = model.fit_generator(train_flow,\n",
        "                    steps_per_epoch=len(X_train) / batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=test_flow,\n",
        "                    validation_steps=len(X_test) / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtw5sGh_ocNh"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "'''Save the Model\n",
        "Saving our model’s architecture into JSON and model’s weight into .h5.\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''esting the Model using Webcam Feed\n",
        "In this part, we will test our model in real-time using face detection.\n",
        "\n",
        "Loading the saved model\n",
        "Let’s start by loading the trained model architecture and weights so that it can be used further to make predictions.\n",
        "\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model = model_from_json(open(\"model_arch.json\", \"r\").read())\n",
        "model.load_weights('model.h5')\n",
        "Loading Har-Cascade for Face Detection\n",
        "We are using Haar-cascade for the detection position of faces and after getting position we will crop the faces.\n",
        "\n",
        "haarcascade_frontalface_default can be downloaded using the link.\n",
        "\n",
        "import cv2\n",
        "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "Read Frames and apply Preprocessing using OpenCV\n",
        "Use OpenCV to read frames and for image processing.\n",
        "\n",
        "cap=cv2.VideoCapture(0)while cap.isOpened():\n",
        "    res,frame=cap.read()height, width , channel = frame.shapegray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_haar_cascade.detectMultiScale(gray_image )\n",
        "    try:\n",
        "        for (x,y, w, h) in faces:\n",
        "            cv2.rectangle(frame, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n",
        "            roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n",
        "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
        "            image_pixels = img_to_array(roi_gray)\n",
        "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
        "            image_pixels /= 255\n",
        "            predictions = model.predict(image_pixels)\n",
        "            max_index = np.argmax(predictions[0])\n",
        "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "            emotion_prediction = emotion_detection[max_index]'''"
      ],
      "metadata": {
        "id": "nDRG6SSZE8Pk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0OrmclsbgYF5L35ibjEtW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}